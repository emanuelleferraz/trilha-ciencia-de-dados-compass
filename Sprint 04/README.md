# üìä Entrega - Sprint 04

Nessa Sprint 04, a trilha foi dividida entre 2 cursos, sendo eles: **Machine Learning e Data Science com Python** e **Machine Learning com Amazon AWS SageMaker**.

## Link do V√≠deo

# üìù Exerc√≠cios

### Curso de Machine Learning e Data Science com Python
Durante o curso, n√£o foram realizados diretamente exerc√≠cios, dessa forma, achei interessante fazer o upload dos arquivos notebooks referentes as aulas. Segue abaixo os arquivos divididos pela se√ß√£o do curso:
1. **Regress√£o Linear**:

    ‚û°Ô∏è Confira o [notebook](./exercicios/curso%20machine%20learning/regressao-linear.ipynb).

2. **Outros tipos de Regress√£o**:

    ‚û°Ô∏è Confira o [notebook](./exercicios/curso%20machine%20learning/outras-regressoes.ipynb).    

3. **Algoritmo Apriori**:

    ‚û°Ô∏è Confira o [notebook](./exercicios/curso%20machine%20learning/apriori.ipynb).

4. **Agrupamento**:

    ‚û°Ô∏è Confira o [notebook](./exercicios/curso%20machine%20learning/agrupamento.ipynb).

5. **Sele√ß√£o de Atributos**:

    ‚û°Ô∏è Confira o [notebook](./exercicios/curso%20machine%20learning/selecao-atributos.ipynb).

6. **Redu√ß√£o da Dimensionalidade (PCA)**:

    ‚û°Ô∏è Confira o [notebook](./exercicios/curso%20machine%20learning/reducao-dimensionalidade.ipynb).

5. **Detec√ß√£o de Outliers**:

    ‚û°Ô∏è Confira o [notebook](./exercicios/curso%20machine%20learning/outliers.ipynb).


### Machine Learning com Amazon AWS SageMaker
Durante o curso foram realizados alguns exerc√≠cios, por quest√£o de tempo n√£o foi poss√≠vel fazer todos eles, ent√£o achei interessante inserir os arquivos notebooks das aulas:

1. **Regress√£o com Linear Learner e XGBoost**:

    ‚û°Ô∏è Confira o [notebook do linear learner](./exercicios/curso%20amazon%20sagemaker/01%20-%20regressao/learner-houses.ipynb).

    ‚û°Ô∏è Confira o [notebook do xgboost](./exercicios/curso%20amazon%20sagemaker/01%20-%20regressao/xgboost.ipynb).

2. **Classifica√ß√£o com Linear Learner**:

    ‚û°Ô∏è Confira o notebook de [linear learner](./exercicios/curso%20amazon%20sagemaker/02%20-%20classificacao/learner-census.ipynb).

3. **S√©ries Temporais com DeepAr**:

    ‚û°Ô∏è Confira o notebook de [s√©ries temporais](./exercicios/curso%20amazon%20sagemaker/03%20-%20serie%20temporal/bikes-deepar.ipynb).

4. **Detec√ß√£o de Outliers com Random Cut Forest**:

    ‚û°Ô∏è Confira o notebook de [detec√ß√£o de outliers](./exercicios/curso%20amazon%20sagemaker/04%20-%20outliers/bike-random-cut.ipynb).

5. **PCA com Agrupamento**:

    ‚û°Ô∏è Confira o notebook de [redu√ß√£o de dimensionalidade com agrupamento](./exercicios/curso%20amazon%20sagemaker/05%20-%20pca%20com%20agrupamento/pca-agrupamento-credit.ipynb).

6. **TensorFlow**:

    ‚û°Ô∏è Confira o notebook de [tensorflow cl√°ssico](./exercicios/curso%20amazon%20sagemaker/06%20-%20tensorflow/tensorflow-classico.ipynb).

# üìÇ Evid√™ncias

### Curso de Machine Learning e Data Science com Python
Neste curso, foram aprofundados conceitos e algoritmos de Machine Learning de regress√£o linear e outros tipos de regress√£o at√© detec√ß√£o de outliers:

- **Regress√£o**
  - A **regress√£o linear** √© uma das t√©cnicas estat√≠sticas utilizada para 'modelar' a rela√ß√£o entre uma vari√°vel dependente (objetivo) e uma ou mais vari√°veis independentes (previsores). Uma coisa interessante a se destacar nesse curso, √© que o instrutor deu foco na equa√ß√£o da reta, podendo at√© mesmo visualizar os valores da reta buscados na 'unha' utilizando alguns par√¢metros como mostrado na imagem abaixo, e al√©m disso √© comparado com o resultado da previs√£o buscado pelo m√©todo de previs√£o:
  
  <p align="center">
    <img src="./evidencias/curso machine learning/reg-formula.png" alt="Gr√°fico de Regress√£o" width="400">
  </p>

  Outra coisa interessante a se observar √© a gera√ß√£o de um gr√°fico de visualiza√ß√£o dos residuais, esse gr√°fico mostra o quanto os dados est√£o afastados da linha de regress√£o:

  <p align="center">
    <img src="./evidencias/curso machine learning/grafico1.png" alt="Gr√°fico de Regress√£o" width="400">
    <img src="./evidencias/curso machine learning/grafico2.png" alt="Gr√°fico de Residuais" width="400">
  </p>

  - Outro ponto mostrado no curso foi a gera√ß√£o do gr√°fico de correla√ß√£o que mostra o quanto as vari√°veis est√£o associadas. Esse gr√°fico √© interessante para analisar quais vari√°veis dependentes podem ser utilizadas para fazer a previs√£o. A correla√ß√£o pode ser obtido com o comando ``` base_de_dados.corr() ```.

  <p align="center">
    <img src="./evidencias/curso machine learning/grafico3.png" alt="Gr√°fico de Correla√ß√£o" width="400">
  </p>

  - No curso s√£o abordados outros tipos de regress√µes, a primeira delas √© a **regress√£o polinomial** que √© uma extens√£o da regress√£o linear, usada para modelar rela√ß√µes n√£o lineares entre as vari√°veis. Nessa regress√£o √© poss√≠vel capturar curvaturas e padr√µes mais complexos nos dados, que n√£o podem ser explicados por uma simples linha reta, como mostrado no gr√°fico abaixo. Outro tipo de regress√£o mostrado √© a com **√°rvores de decis√£o** na qual utiliza um modelo baseado em divis√µes (splits) sucessivas dos dados (em formato de √°rvore), formando intervalos onde a predi√ß√£o √© constante. Para cada intervalo, a m√©dia dos valores de `Y` dentro do intervalo √© utilizada como predi√ß√£o.

 <p align="center">
    <img src="./evidencias/curso machine learning/grafico4-poly.png" alt="Gr√°fico de Regress√£o Polinomial" width="400">
    <img src="./evidencias/curso machine learning/grafico5-dt.png" alt="Gr√°fico de Regress√£o com DT" width="400">
  </p>

Nota-se que no algoritmo de √°rvore de decis√£o os dados s√£o exatamente iguais, isso acontece por conta dos splits (segmenta√ß√£o dos dados em blocos).

  - Foram trabalhados outros algoritmos para regress√£o como regress√£o com suporte a vetor e redes neurais, mas o 'melhor' algoritmo foi o de random forest. O **Random Forest** √© um algoritmo de aprendizado de m√°quina baseado em um conjunto de √°rvores de decis√£o fazendo ao final uma m√©dia e escolhendo o melhor modelo. Abaixo segue o gr√°fico gerado desse algoritmo:

  <p align="center">
    <img src="./evidencias/curso machine learning/grafico6-rf.png" alt="Gr√°fico de Correla√ß√£o" width="400">
  </p>

Durante os testes, o random forest foi o algoritmo de regress√£o com os melhores resultados no dataset utilizado. Isso acontece pois sua capacidade de capturar tanto padr√µes globais quanto locais nos dados, gerando predi√ß√µes muito precisas.


- **Apriori**  
  - **Apriori** √© um algoritmo utilizado para minera√ß√£o de regras de associa√ß√£o em grandes conjuntos de dados. Ele √© muito utilizado em cen√°rios como an√°lise de cestas de mercado (market basket analysis), na qual o objetivo √© identificar itens que frequentemente aparecem juntos em transa√ß√µes. A imagem abaixo mostra as regras de associa√ß√£o escolhidas pelo algoritmo no dataset das aulas, bem como a constru√ß√£o do novo dataframe com as regras:

  <p align="center">
    <img src="./evidencias/curso machine learning/apriori.png" alt="Apriori" width="400">
    <img src="./evidencias/curso machine learning/apriori2.png" alt="Apriori" width="400">
    <img src="./evidencias/curso machine learning/aprioridt.png" alt="Apriori" width="400">
  </p>


- **Agrupamento**  
  - O agrupamento √© uma t√©cnica para agrupar dados que s√£o semelhantes, nesse sentido, existem alguns algoritmos que foram mostrados no curso, sendo o primeiro deles o **K-Means**.
    - **K-Means** √© um agrupamento n√£o supervisionado que divide os dados em `K` clusters. Ele busca minimizar a vari√¢ncia dentro de cada cluster, agrupando pontos que s√£o mais semelhantes entre si. Abaixo, segue como foi feita a clusteriza√ß√£o com esse algoritmo. Al√©m disso, quando n√£o se tem certeza sobre a quantidade de centroides que ser√£o criados, pode-se usar o m√©todo **WCSS**, nesse m√©todo, o ponto onde a redu√ß√£o do WCSS come√ßa a diminuir significativamente √© escolhido como o n√∫mero √≥timo de clusters.

  <p align="center">
    <img src="./evidencias/curso machine learning/kmeans.png" alt="K-Means" width="400">
    <img src="./evidencias/curso machine learning/kmeans-wcss.png" alt="WCSS" width="400">
  </p>

  Abaixo segue os gr√°ficos referentes a codifica√ß√£o do K-Means e do WCSS acima:

  <p align="center">
    <img src="./evidencias/curso machine learning/grafico-kmeans.png" alt="Gr√°fico K-Means" width="400">
    <img src="./evidencias/curso machine learning/grafico-kmeans-wcss.png" alt="Gr√°fico WCSS" width="400">
  </p>

  - Outros dois algoritmos famosos s√£o o **DBSCAN** e o **Agrupamento Hier√°rquico**, na qual o primeiro realiza o agrupamento baseado em densidade. Ele agrupa pontos densamente conectados e marca os pontos que n√£o pertencem a nenhum cluster como ru√≠do. O segundo cria uma √°rvore de agrupamentos chamada **dendrograma**:

   <p align="center">
    <img src="./evidencias/curso machine learning/dbscan.png" alt="DBSCAN" width="400">
    <img src="./evidencias/curso machine learning/grafico-dendrograma.png" alt="Dendrograma" width="400">
   </p>

   Ainda sobre agrupamento, ao final das aulas, foram feitas algumas compara√ß√µes entre os algoritmos e a clusteriza√ß√£o de ambos. Os resultados est√£o apresentados nos gr√°ficos abaixo sendo o DBSCAN com desempenho superior aos outros dois.

   <p align="center">
    <img src="./evidencias/curso machine learning/graf-comp-hierarquico.png" alt="Hier√°rquico" width="300">
    <img src="./evidencias/curso machine learning/graf-comp-kmeans.png" alt="K-Means" width="300">
    <img src="./evidencias/curso machine learning/graf-comp-dbscan.png" alt="DBSCAN" width="300">
   </p>

- **Sele√ß√£o de Atributos**  
  - A sele√ß√£o de atributos √© uma t√©cnica usada para escolher os atributos mais relevantes em um conjunto de dados, removendo aqueles que n√£o contribuem significativamente para o modelo. Nas aulas do curso, foi utilizado o crit√©rio da **vari√¢ncia** para identificar os atributos mais importantes:

   <p align="center">
    <img src="./evidencias/curso machine learning/atributos1.png" alt="Atributos" width="400">
    <img src="./evidencias/curso machine learning/atributos2.png" alt="Atributos" width="400">
   </p>

- **Outliers**  
  - A detec√ß√£o de outliers √© uma etapa fundamental na an√°lise de dados, pois valores at√≠picos podem distorcer as an√°lises e comprometer a performance de modelos de machine learning. Nas aulas foram utilizadas tr√™s t√©cnicas para detec√ß√£o de outliers, sendo: Boxplot, Gr√°fico de Dispers√£o e algoritmo PYOD.

   <p align="center">
    <img src="./evidencias/curso machine learning/boxplot.png" alt="Outliers" width="300">
    <img src="./evidencias/curso machine learning/grafico-dispersao.png" alt="Outliers" width="300">
    <img src="./evidencias/curso machine learning/outlier-pyod.png" alt="Outliers" width="300">
   </p>

### Curso de Machine Learning com Amazon AWS SageMaker

Nesse curso, os conceitos mais importantes de Machine Learning foram aplicados na pr√°tica utilizando a Amazon AWS SageMaker:

- **Regress√£o**  
  - Nesse curso, a regress√£o linear foi aplicada utilizando os algoritmos de **Linear Learner** e **XGBoost**, segue abaixo a parametriza√ß√£o e o treinamento de ambos:

  <p align="center">
    <img src="./evidencias/curso amazon sagemaker/regressao.png" alt="linear learner" width="400">
    <img src="./evidencias/curso amazon sagemaker/regressaoxg.png" alt="xgboost" width="400">
   </p>

  - No SageMaker, a utiliza√ß√£o da regress√£o linear com o Linear Learner e o XGBoost comparado com os m√©todos comuns fora da AWS oferece alta efici√™ncia e facilidade para resolver problemas de regress√£o. O Linear Learner √© ideal para problemas lineares, pois √© simples de interpretar, enquanto o XGBoost √© mais poderoso para lidar com rela√ß√µes n√£o-lineares e intera√ß√µes complexas entre vari√°veis. Ambos s√£o otimizados para treinar rapidamente em grandes volumes de dados e permitem ajustes personalizados para melhorar a performance do modelo. Al√©m disso, sua integra√ß√£o no SageMaker facilita o pr√©-processamento, a tunagem autom√°tica de hiperpar√¢metros e a implanta√ß√£o de modelos, tornando o processo mais √°gil e eficiente.

    E por falar em tunagem...

    - **Tunning**  
        - O Tuning consiste em buscar os valores ideais para os hiperpar√¢metros de um modelo de machine learning, com o objetivo de melhorar sua performance. Durante o curso no SageMaker foram criados v√°rios trabalhos de treinamento em paralelo, avaliando os modelos com base em uma m√©trica de performance (RMSE) e identificando a melhor configura√ß√£o. Abaixo segue as melhores configura√ß√µes obtidas com o dataset:

        <p align="center">
          <img src="./evidencias/curso amazon sagemaker/tunning.png" alt="Tuning" width="500">
        </p>

        Importante ressaltar que nesse caso em espec√≠fico os valores do processo de tunagem n√£o foram melhores que os dos algoritmos anteriores, mas isso acontece pelo fato de que o processo foi feito uma √∫nica vez. Para encontrar as melhores configura√ß√µes poss√≠veis √© interessante realizar esse processo mais vezes.

- **Classifica√ß√£o**  
  - Para fazer a classifica√ß√£o, tamb√©m foi utilizado o algoritmo de **Linear Learner** com a diferen√ßa de que, nesse caso, a previs√£o ser√° de uma classe.

  <p align="center">
    <img src="./evidencias/curso amazon sagemaker/deploy-classificacao.png" alt="Tuning" width="500">
  </p>

  Um ponto interessante do SageMaker, √© que h√° a possibilidade de fazer o deploy do modelo, ao chamar o m√©todo de deploy(), o modelo e o endpoint s√£o criados.

- **S√©ries Temporais**  
  - Nesse curso, os dados foram treinados como s√©ries temporais utilizando o algoritmo de DeepAR.

  <p align="center">
    <img src="./evidencias/curso amazon sagemaker/series-temporais.png" alt="Series Temporais" width="400">
    <img src="./evidencias/curso amazon sagemaker/grafico-series.png" alt="Gr√°fico de Series Temporais" width="400">
   </p>

   O DeepAR √© um modelo baseado em redes neurais recorrentes (RNNs), especialmente projetado para prever s√©ries temporais, permitindo que ele capture padr√µes complexos, sazonais e tend√™ncias em v√°rias s√©ries simultaneamente.

- **PCA com Agrupamento**  
  - No curso, a redu√ß√£o da dimensionalidade foi feita com o PCA e logo em seguida foi utilizado o K-Means para a clusteriza√ß√£o. Abaixo est√° o resultado representado graficamente:

  <p align="center">
    <img src="./evidencias/curso amazon sagemaker/pca-agrupamento.png" alt="PCA e K-Means" width="400">
  </p>

- **Outliers**
  - O processo de detec√ß√£o de outliers no curso foi feito com o algoritmo de Random Cut Forest.

  <p align="center">
    <img src="./evidencias/curso amazon sagemaker/outlier-randomcut.png" alt="Random Cut Forest" width="530">
    <img src="./evidencias/curso amazon sagemaker/outlier-randomcut2.png" alt="Random Cut Forest" width="400">
  </p>

  Para encontrar os outliers (pontos em preto no gr√°fico), foi usado a regra comum da m√©dia e desvio padr√£o que √© verificar se os valores est√£o fora de um intervalo de 3 desvios padr√£o (3œÉ) acima da m√©dia.
  

## üèÜ Certificados

Segue abaixo os certificados obtidos nos cursos da Sprint 04:

1. **Machine Learning com Amazon AWS SageMaker**
![Certificado 1](./certificados/Certificado%20Machine%20Learning%20na%20AWS%20SageMaker.jpg)  

N√£o foi poss√≠vel obter o certificado do primeiro curso de Machine Learning com Python, uma vez que foi determinado completar apenas algumas se√ß√µes do curso. 
