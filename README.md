# Est√°gio - AWS Cloud Data Science Compass Uol

## üë©‚Äçüíª Apresenta√ß√£o

Ol√°! Meu nome √© **Emanuelle Ferraz**, natural de **Santo Ant√¥nio do Jacinto-MG** e atualmente residente em **Jo√£o Monlevade-MG**. Estou cursando o 6¬∫ per√≠odo de **Sistemas de Informa√ß√£o** na **Universidade Federal de Ouro Preto (UFOP)**, onde tenho desenvolvido minha paix√£o pela √°rea de tecnologia.


### üéì Experi√™ncias Acad√™micas

Participei de projetos acad√™micos focados em contribuir com a comunidade, sendo um projeto de extens√£o e outro de desenvolvimento institucional, como:
 - **Projeto de Acessibilidade Web**, contribuindo com corre√ß√µes e melhorias de p√°ginas (HTML) da UFOP para torn√°-las mais acess√≠veis.  
    - Segui diretrizes renomadas como **WCAG** (Web Content Accessibility Guidelines) e **e-MAG** (Modelo de Acessibilidade em Governo  Eletr√¥nico), aplicando os padr√µes para garantir inclus√£o digital.
 - **Projeto de Extens√£o Bom de Bola, Bom na Escola**: Atuei promovendo o acesso a oportunidades esportivas para a comunidade de **Jo√£o Monlevade**, com foco no p√∫blico infantil e juvenil. Al√©m disso, participei do acompanhamento do desempenho escolar dos participantes, incentivando o equil√≠brio entre estudos e esportes.

### üíª Experi√™ncias Pessoais

 - Desenvolvi diversos projetos pessoais que abrangem tecnologias **front-end**, como **HTML**, **CSS** e **JavaScript**.  
 - No **back-end**, utilizei linguagens como **Node.js**, **Python** e **PHP**, al√©m de frameworks como **Flask** e **Laravel**.  
 - Integrei APIs em aplica√ß√µes para criar solu√ß√µes completas e funcionais.


Hobbies:
 - ‚öΩ Praticar, assistir e jogar futebol.
 - üßß Gosto de assistir conte√∫dos de Design Gr√°fico, uma vez que fa√ßo logos e artes para m√≠dias sociais quando me pedem.
 - üìñ Gosto de ler, principalmente conte√∫dos de fantasia.
 - üíª Tento manter o aprendizado constante em √°reas como desenvolvimento de software e tecnologias emergentes.

### Video de Apresenta√ß√£o
 Assista ao [v√≠deo](Apresenta√ß√£o-Compass.mp4) para conhecer mais sobre mim! üòä

## Sprints - Entregas

1. [Sprint 1](./Sprint%2001/README.md)
2. [Sprint 2](./Sprint%2002/README.md)
3. [Sprint 3](./Sprint%2003/README.md)
4. [Sprint 4](./Sprint%2004/README.md)



## üìö Conte√∫do de Aprendizado
 ### Sprint 1
 Durante a Sprint 01, foram explorados os seguintes conte√∫dos:

- **Metodologia √Ågil com foco no Scrum**: Aprendi os principais conceitos e pr√°ticas, como os pap√©is (Scrum Master, Product Owner e Time de Desenvolvimento), eventos (Daily, Sprint Review, Sprint Retrospective, etc.) e artefatos (Product Backlog, Sprint Backlog e Incremento).
- **Git e GitHub**: Aprendi os principais comandos git, como `init`, `add`, `commit`, `status`, `push`, `pull` e etc. Al√©m de outros comandos para realizar altera√ß√µes e conceitos de branch e merge. Ademais, foi ensinado como usar o GitHub para versionamento de c√≥digo e colabora√ß√£o em equipe.
- **Curso de Python**: Durante este curso, aprendi desde os fundamentos como tipos de dados e vari√°veis at√© conceitos avan√ßados da linguagem como poo, fun√ß√µes avan√ßadas, manipula√ß√£o de arquivos e gr√°ficos. Segue o resumo dos conte√∫dos aprendidos divididos por se√ß√£o:

  
| **Se√ß√£o** | **Conte√∫do Aprendido**                              | **Descri√ß√£o Resumida**                                            |
|-----------|-----------------------------------------------------|-------------------------------------------------------------------|
| Se√ß√£o 2   | Vari√°veis e Tipos de Dados                          | Fundamentos da linguagem, incluindo vari√°veis e tipos b√°sicos.   |
| Se√ß√£o 3   | Estruturas de Controle: Condicionais e Loopings     | Estruturas para controle de fluxo, como `if-else`, `for` e `while`.   |
| Se√ß√£o 4   | M√≥dulos e Estruturas de Dados                      | Listas, tuplas, dicion√°rios, sets, manipula√ß√£o de strings, e bibliotecas como `math`, `random` e `datetime`. |
| Se√ß√£o 5   | Fun√ß√µes e Tratamento de Exce√ß√µes                   | Escopo de fun√ß√µes, fun√ß√µes especiais (`map()`, `filter()`, `reduce()`), recursividade e tratamento de erros. |
| Se√ß√£o 6   | Recursos Avan√ßados                                 | Manipula√ß√£o de arquivos de texto e CSV, compreens√£o de listas, m√≥dulo `os` e introdu√ß√£o √† Programa√ß√£o Orientada a Objetos. |
| Se√ß√£o 7   | Visualiza√ß√£o de Dados com Matplotlib               | Cria√ß√£o de gr√°ficos como `plot`, `bar` e `pie` usando a biblioteca Matplotlib. |


- **Curso de Pandas**: Durante este curso, explorei as funcionalidades da biblioteca Pandas para manipula√ß√£o e an√°lise de dados, abordando desde conceitos b√°sicos at√© t√©cnicas mais avan√ßadas. O aprendizado foi dividido em tr√™s se√ß√µes principais:

  - **Se√ß√£o 1: Series**  
    - Cria√ß√£o de Series, Fatiamento de Series, Acesso com `loc` e `iloc`, Ordena√ß√£o, Agrupamento, Concatena√ß√£o, Contagem, Filtros, Opera√ß√µes Matem√°ticas, Opera√ß√µes com String, Tratamento de Valores Faltantes e Aplica√ß√£o de Fun√ß√µes.

  - **Se√ß√£o 2: DataFrame**  
    - Cria√ß√£o de DataFrames, Acesso com `loc` e `iloc`, Explora√ß√£o de DataFrames, Remo√ß√£o de Linhas e Colunas, Tratamento de Linhas Duplicadas, Valores Faltantes, Ordena√ß√£o, Filtragem, Renomea√ß√£o e Reordena√ß√£o de Colunas, Cria√ß√£o de Novas Colunas, Trabalhando com Colunas Categ√≥ricas, Agrega√ß√£o, Agrupamento, Tabelas Pivot, Jun√ß√£o de DataFrames e Convers√£o para Data.

  - **Se√ß√£o 3: Gr√°ficos**  
    - Utilizando a integra√ß√£o com **Matplotlib**, criei diferentes tipos de gr√°ficos, como: Barra, Pizza, Linha, Subplots, Dispers√£o e Histogramas.

 ### Sprint 2
 Durante a Sprint 02, foram explorados os seguintes conte√∫dos:

 - **Curso de SQL para An√°lise de Dados**: Neste curso, aprendi desde os comandos b√°sicos para consultas, como `SELECT`, `FROM`, `WHERE`, at√© conte√∫dos avan√ßados, como *joins*, *subqueries*, *tratamento de dados* e *fun√ß√µes de agrega√ß√£o*. Segue o resumo dos conte√∫dos aprendidos divididos por se√ß√£o:

 | **Se√ß√£o** | **Conte√∫do Aprendido**                              | **Descri√ß√£o Resumida**                                            |
|-----------|-----------------------------------------------------|-------------------------------------------------------------------|
| Se√ß√£o 02  | Configura√ß√£o do Ambiente                            | Instala√ß√£o do PostgreSQL e cria√ß√£o do banco de dados.            |
| Se√ß√£o 03  | Comandos B√°sicos                                    | Comandos fundamentais como `SELECT`, `FROM`, `WHERE`, `DISTINCT`, `LIMIT` e `ORDER BY`. |
| Se√ß√£o 04  | Operadores Aritm√©ticos, L√≥gicos e de Compara√ß√£o     | **Aritm√©ticos**: `+`, `-`, `*`, `/`, `%`. **Compara√ß√£o**: `=`, `<>`, `>`, `<`, `>=`, `<=`. **L√≥gicos**: `AND`, `OR`, `NOT`, `BETWEEN`, `LIKE`, `ILIKE`, `IS NULL`. |
| Se√ß√£o 05  | Fun√ß√µes de Agrega√ß√£o                                | Fun√ß√µes como `MIN`, `MAX`, `AVG`, `COUNT`, `SUM`, al√©m de `GROUP BY` e `HAVING`. |
| Se√ß√£o 06  | Joins                                               | Tipos de *joins*: `INNER JOIN`, `LEFT JOIN`, `RIGHT JOIN`, `FULL JOIN`. |
| Se√ß√£o 07  | Union                                               | Combina√ß√µes de conjuntos de dados com `UNION` e `UNION ALL`.     |
| Se√ß√£o 08  | Subqueries                                          | Subqueries aplicadas em diferentes contextos: no `WHERE`, com `WITH`, no `FROM` e no `SELECT`. |
| Se√ß√£o 09  | Tratamento de Dados                                 | **Convers√µes**: com operador `::` ou `CAST`. **Gerais**: `CASE WHEN`, `COALESCE`. **Texto**: `UPPER`, `LOWER`, `TRIM`, `REPLACE`. **Datas**: `INTERVAL`, `DATE_TRUNC`, `EXTRACT`, fun√ß√µes personalizadas como `DATEDIFF`. |
| Se√ß√£o 10  | Manipula√ß√£o de Tabelas                              | **Tabelas**: `CREATE TABLE`, `CREATE TABLE AS` (a partir de query), `DROP TABLE`. **Linhas**: `INSERT INTO VALUES`, `DELETE`, `UPDATE SET`. **Colunas**: `ALTER TABLE`, `UPDATE SET` e `DELETE`. |

- **Estat√≠stica para An√°lise de Dados**: Nesse curso foram ensinados todos os conceitos estat√≠sticos necess√°rios para realizar an√°lises de dados. Desde conceitos fundamentais da estat√≠stica (medidas centrais) at√© conceitos probabil√≠sticos:


- Se√ß√£o 3: Prepara√ß√£o, Organiza√ß√£o e Estrutura√ß√£o dos Dados  
  - Tratamento e organiza√ß√£o dos dados do ENEM 2019 no estado de S√£o Paulo.  
  - Utiliza√ß√£o de **Jupyter Notebook** e **Pandas** para:  
    - Limpeza e tratamento de dados.  
    - Exporta√ß√£o do dataset tratado para an√°lise.  

- Se√ß√£o 4: Fundamentos de Estat√≠stica  
  - **Conceitos gerais** de Estat√≠stica e Amostragem:  
    - Amostragem Simples, Estratificada e Sistem√°tica.  
  - **Distribui√ß√£o de Frequ√™ncias**:  
    - Constru√ß√£o de tabelas, histogramas e an√°lise visual dos dados.  
  - **Medidas de Tend√™ncia Central**:  
    - M√©dia: Soma dos valores dividida pelo n√∫mero total de elementos.  
    - Mediana: Valor central de um conjunto de dados ordenado.  
    - Moda: Valor mais frequente em um conjunto de dados.  
  - **Medidas de Dispers√£o e Posi√ß√£o**:  
    - Vari√¢ncia: Mede o quanto os dados se desviam da m√©dia.  
    - Desvio Padr√£o: Raiz quadrada da vari√¢ncia, usado para medir a dispers√£o.  
    - Intervalo Interquartil (IQR): Diferen√ßa entre o terceiro e o primeiro quartil (Q3 - Q1).  
    - Quartis: Dividem os dados em quatro partes iguais.  
    - Limites Inferior e Superior: Identificam poss√≠veis outliers com base no IQR.  
  - **An√°lise de Outliers**:  
    - Utiliza√ß√£o de gr√°ficos como Boxplot para identificar valores extremos.  

- Se√ß√£o 5: Estat√≠stica Probabil√≠stica para An√°lise de Dados  
  - **Probabilidade**:  
    - Conceitos b√°sicos, como eventos e espa√ßo amostral.  
  - **Teorema de Bayes**:  
    - Ferramenta para calcular probabilidades condicionais.  
  - **Probabilidade da Uni√£o, Intersec√ß√£o e Condicional**:  
    - Aplica√ß√£o em diferentes cen√°rios para calcular rela√ß√µes entre eventos.  
  - **Distribui√ß√£o de Probabilidades**:  
    - Discreta: Exemplo, distribui√ß√£o binomial.  
    - Cont√≠nua: Exemplo, distribui√ß√£o normal.  
  - **Teste de Normalidade**:  
    - Verifica√ß√£o de conformidade com a distribui√ß√£o normal para os dados analisados.  

- Se√ß√£o 6: Testes Param√©tricos e N√£o Param√©tricos  
  - **Intervalo de Confian√ßa**:  
    - Estimativa do intervalo onde o par√¢metro populacional est√° localizado.  
  - **Distribui√ß√µes t e Qui-Quadrado**:  
    - Distribui√ß√µes utilizadas em an√°lises de vari√°veis cont√≠nuas e categ√≥ricas.  
  - **Testes de Hip√≥teses**:  
    - Teste Z: Aplicado para m√©dias de grandes amostras.  
    - Teste t para duas amostras independentes (**Teste T de Student**).  
  - **Testes N√£o Param√©tricos**:  
    - Teste Mann-Whitney: Compara√ß√£o de dois grupos independentes.  
    - Teste de Wilcoxon: Compara√ß√£o de pares de dados.  
  - **Correla√ß√£o Linear e Regress√£o Linear**:  
    - Estudo das rela√ß√µes entre vari√°veis e a constru√ß√£o de modelos preditivos.  

 ### Sprint 3
  Durante a Sprint 03, foram explorados conceitos de Intelig√™ncia Artificial e Machine Learning atrav√©s do seguinte curso:

  - **Curso de IA e Machine Learning**: Ao longo do curso, foram abordados desde conceitos introdut√≥rios at√© t√≥picos avan√ßados, com se√ß√µes estruturadas para facilitar o aprendizado, sendo:

  - **Se√ß√£o 01: Introdu√ß√£o**
    - **O que √© Machine Learning**: Campo da IA que ensina computadores a aprender padr√µes e realizar tarefas sem serem explicitamente programados.
    - **Estrutura dos Dados**: Organiza√ß√£o e formata√ß√£o dos dados para an√°lise e processamento.
    - **Aplica√ß√µes de ML**: Reconhecimento facial, detec√ß√£o de fraudes, previs√µes financeiras, diagn√≥stico m√©dico, entre outros.



- **Se√ß√£o 02: Fundamentos de Machine Learning**
  - **Classifica√ß√£o**: M√©todo de aprendizado supervisionado onde o objetivo √© prever a categoria de dados.

  **Matriz de Confus√£o**
  | **Termo**               | **Defini√ß√£o**                                                                                   |
  |-------------------------|-----------------------------------------------------------------------------------------------|
  | **Verdadeiros Positivos** | Previs√µes corretas de uma classe positiva.                                                     |
  | **Verdadeiros Negativos** | Previs√µes corretas de uma classe negativa.                                                     |
  | **Falsos Positivos**      | Previs√µes incorretas onde uma classe negativa foi prevista como positiva.                      |
  | **Falsos Negativos**      | Previs√µes incorretas onde uma classe positiva foi prevista como negativa.                      |

  **M√©tricas de Avalia√ß√£o de Performance**
  | **M√©trica**       | **Defini√ß√£o**                                                                                     |
  |-------------------|-------------------------------------------------------------------------------------------------|
  | **Acur√°cia**      | Propor√ß√£o de previs√µes corretas em rela√ß√£o ao total.                                            |
  | **Precis√£o**      | Propor√ß√£o de previs√µes positivas corretas.                                                      |
  | **Recall**        | Propor√ß√£o de positivos corretamente identificados.                                              |
  | **F1 Score**      | M√©dia harm√¥nica entre precis√£o e recall.                                                        |
  | **Especificidade**| Propor√ß√£o de negativos corretamente identificados.                                              |

  - **Codifica√ß√£o de Categorias**: T√©cnicas como Label Encoding e One Hot Encoding para representar vari√°veis categ√≥ricas.
  - **Dimensionamento de Caracter√≠sticas**: Normaliza√ß√£o e padroniza√ß√£o dos dados para melhor performance dos modelos.
  - **Regras de Associa√ß√£o**: Descoberta de rela√ß√µes √∫teis entre vari√°veis em grandes conjuntos de dados.



- **Se√ß√£o 03: Estudo de Algoritmos de Machine Learning**
  - **Correla√ß√£o e Regress√£o Linear**: Identifica√ß√£o de rela√ß√µes entre vari√°veis.
  - **Naive Bayes**: Classificador probabil√≠stico baseado no Teorema de Bayes.
  - **√Årvore de Decis√£o**: Modelo baseado em decis√µes sequenciais.
    - **Ganho e Entropia**: M√©tricas para decidir divis√µes na √°rvore.
  - **Random Forest**: Conjunto de √°rvores de decis√£o que melhora a precis√£o.
  - **KNN**: Classifica√ß√£o baseada nos vizinhos mais pr√≥ximos.
  - **KMeans e Clusteriza√ß√£o**: Agrupamento de dados baseado na similaridade.



- **Se√ß√£o 04: T√≥picos Avan√ßados em Machine Learning**
  - **Engenharia de Atributos**: Cria√ß√£o de novas vari√°veis a partir dos dados existentes.
  - **PCA (An√°lise de Componentes Principais)**: Redu√ß√£o de dimensionalidade dos dados.
  - **Avalia√ß√£o de Modelos**: Uso de intervalos de confian√ßa e testes de hip√≥teses para compara√ß√£o.
  - **T√©cnicas Avan√ßadas de Clusters**: Uso de m√©todos como elbow, silhouette e gap.
  - **Classifica√ß√£o Multi Label**: Modelos para prever m√∫ltiplas classes ao mesmo tempo.
  - **Dados Desbalanceados**: M√©todos como oversampling, undersampling e SMOTE.



- **Se√ß√£o 05: Redes Neurais, Deep Learning e Vis√£o Computacional**
  - **Redes Neurais**: Modelos inspirados no funcionamento do c√©rebro humano.
  - **Perceptron**: Unidade b√°sica de uma rede neural.
  - **Deep Learning**: Redes neurais profundas para resolver problemas complexos.
  - **RNA com Keras**: Implementa√ß√£o de redes neurais utilizando a biblioteca Keras.

  **Vis√£o Computacional**
  | **Etapa**           | **Descri√ß√£o**                                            |
  |---------------------|----------------------------------------------------------|
  | **Convolution**     | Extra√ß√£o de caracter√≠sticas por meio de filtros.         |
  | **Pooling**         | Redu√ß√£o dimensional para evitar overfitting.             |
  | **Flattening**      | Transforma√ß√£o dos dados para entrada na camada densa.    |
  | **Full Connection** | Conex√£o total entre neur√¥nios para previs√£o final.       |

  - **Autoencoders**: Modelos para compress√£o e reconstru√ß√£o de dados.
  - **LSMT**: Redes neurais recorrentes para dados sequenciais.



- **Se√ß√£o 06: Machine Learning Explic√°vel**
  - **Defini√ß√£o**: M√©todos para tornar o comportamento dos modelos mais interpret√°vel.
  - **Modelos White box e Black box**: Explic√°veis versus complexos e opacos.
  - **Ferramentas**: Lime, Eli5, Shap e Interpret para explicabilidade.



- **Se√ß√£o 07: NLP e LLM**
  - **NLP (Processamento de Linguagem Natural)**: T√©cnicas para an√°lise e compreens√£o de texto.
  - **Word Embedding e Transformers**: Representa√ß√£o vetorial de palavras e arquitetura para NLP.
  - **LLMs (Large Language Models)**: Modelos de linguagem como GPT.
  - **Hugging Face**: Ferramentas e modelos pr√©-treinados para NLP.


- **Se√ß√£o 09: Detec√ß√£o de Anomalias**
  - **Outliers**: Dados que se desviam significativamente do padr√£o.
  - **Z-Score**: M√©trica para identificar outliers baseado na m√©dia e desvio padr√£o.
  - **IQR (Intervalo Interquartil)**: M√©todo para detec√ß√£o de outliers com quartis.
  - **LOF**: Identifica√ß√£o de anomalias com base na densidade local.
  - **T√©cnicas de S√©ries Temporais**: M√©todos para an√°lise de dados sequenciais.

### Sprint 4
 Durante a Sprint 04, foram explorados os seguintes conte√∫dos:

 - **Curso Machine Learning com Python**: Neste curso, foram abordados assuntos como previs√µes utilizando regress√£o linear, algoritmo Apriori, agrupamento (clusteriza√ß√£o) com KMeans e DBSCAN, sele√ß√£o de atributos, redu√ß√£o da dimensionalidade (PCA) e detec√ß√£o de outliers.

   - **Se√ß√£o 14: Regress√£o Linear**: Existindo uma rela√ß√£o linear entre as vari√°veis dependente e explanat√≥ria(s), √© um indicativo de   que podemos aplicar a regress√£o linear para prever (n√∫meros). Para encontrar essa rela√ß√£o, √© necess√°rio calcular:
      - Covari√¢ncia: Mede como duas vari√°veis se comportam juntas, indicando se elas t√™m uma rela√ß√£o positiva (aumentam juntas) ou negativa (uma aumenta enquanto a outra diminui).

      - Coeficiente de Correla√ß√£o:Mede a intensidade e a dire√ß√£o da rela√ß√£o linear entre duas vari√°veis. Seus valores variam de -1 (correla√ß√£o negativa perfeita) a +1 (correla√ß√£o positiva perfeita). Um valor pr√≥ximo de 0 indica pouca ou nenhuma rela√ß√£o linear.

      - Coeficiente de Determina√ß√£o: Representa a propor√ß√£o da varia√ß√£o de uma vari√°vel que pode ser explicada pela outra em um modelo de regress√£o. √â o quadrado do coeficiente de correla√ß√£o e varia de 0 a 1, onde 1 indica que 100% da varia√ß√£o √© explicada pelo modelo.
    
    - **Se√ß√£o 15: Outros Tipos de Regress√£o**: Nessa se√ß√£o foram apresentadas outros tipos de regress√£o al√©m da linear, uma vez que podem existir problemas n√£o linearmente separ√°veis. 
      - Regress√£o Polinomial: Extens√£o da regress√£o linear que utiliza termos polinomiais (e.g., x¬≤, x¬≥) para capturar rela√ß√µes n√£o lineares entre as vari√°veis independentes e a vari√°vel dependente.

      - Regress√£o com √Årvores de Decis√£o: T√©cnica baseada em divis√µes sucessivas do espa√ßo de atributos, criando uma estrutura de √°rvore para prever valores cont√≠nuos em vez de classes.

      - Regress√£o com Random Forest: Conjunto de m√∫ltiplas √°rvores de decis√£o treinadas com diferentes subconjuntos dos dados. As previs√µes s√£o combinadas (no caso de regress√£o √© geralmente por m√©dia) para melhorar a precis√£o e reduzir o overfitting.

      - Regress√£o com Vetores de Suporte: Utiliza margens (ou hiperplanos) para prever valores cont√≠nuos, minimizando o erro dentro de uma margem toler√°vel, utilizando o conceito de suporte vetorial.

      - Regress√£o com Rede Neural: Baseada em redes de camadas interconectadas de n√≥s (neur√¥nios), onde os pesos s√£o ajustados durante o treinamento para aprender padr√µes complexos e prever valores cont√≠nuos.

    - **Se√ß√£o 17: Algoritmo Apriori**: O algoritmo Apriori √© uma t√©cnica de aprendizado de m√°quina usada para minera√ß√£o de regras de associa√ß√£o em conjuntos de dados transacionais. Ele identifica padr√µes frequentes nos dados, como itens que frequentemente aparecem juntos em transa√ß√µes, e gera regras baseadas nesses padr√µes (normalmente utilizado em mercados).

    | **M√©trica**   | **Descri√ß√£o**                                                                                     | **F√≥rmula**                                         |
    |---------------|-------------------------------------------------------------------------------------------------|---------------------------------------------------|
    | **Suporte**   | Mede a frequ√™ncia com que um item ou conjunto de itens aparece no banco de dados, representado como uma propor√ß√£o do total de transa√ß√µes. | **Suporte = (Transa√ß√µes com A e B) / Total de transa√ß√µes** |
    | **Confian√ßa** | Mede a probabilidade de um item B ser comprado dado que o item A foi comprado. Avalia a for√ßa da regra. | **Confian√ßa = (Transa√ß√µes com A e B) / (Transa√ß√µes com A)** |
    | **Lift**      | Mede o qu√£o mais prov√°vel √© que dois itens sejam comprados juntos em compara√ß√£o ao que seria esperado se fossem independentes. | **Lift = Confian√ßa / Suporte de B**              |

    - **Se√ß√£o 20: Agrupamento com K-Means**:  
      - O algoritmo K-Means √© utilizado para agrupar dados em *k* clusters com base em semelhan√ßas.  
      - Funcionamento:  
        - Inicializa√ß√£o: Escolha aleat√≥ria de *k* centr√≥ides.  
        - Atribui√ß√£o: Cada ponto de dados √© associado ao centr√≥ide mais pr√≥ximo (geralmente usando a dist√¢ncia Euclidiana).  
        - Atualiza√ß√£o: Recalcula-se a posi√ß√£o dos centr√≥ides como a m√©dia dos pontos atribu√≠dos ao cluster.  
        - Itera√ß√£o: Repete os passos de atribui√ß√£o e atualiza√ß√£o at√© a converg√™ncia ou uma condi√ß√£o de parada.  
      - O n√∫mero ideal de clusters (*k*) pode ser determinado por m√©todos como:  
        - **Elbow Method**: Identifica√ß√£o do ponto onde a redu√ß√£o na in√©rcia (soma das dist√¢ncias ao centr√≥ide) come√ßa a diminuir significativamente.  
        - **Silhouette Score**: Mede a qualidade do agrupamento com base na separa√ß√£o e coes√£o dos clusters.  
      
    - **Se√ß√£o 21: Outros Algoritmos de Agrupamento**:  
      - **Hier√°rquico**:  
        - Cria uma hierarquia de clusters utilizando abordagens aglomerativas (bottom-up) ou divisivas (top-down).  
        - N√£o requer defini√ß√£o pr√©via do n√∫mero de clusters.  
        - Representado por um dendrograma, permitindo a escolha do n√∫mero de clusters com base nos n√≠veis de corte.  
        - Mais adequado para conjuntos de dados menores devido ao custo computacional.  

      - **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**:  
        - Agrupa pontos com base em densidade, formando clusters onde h√° uma concentra√ß√£o significativa de pontos.  
        - Identifica ru√≠dos (outliers) como pontos fora de regi√µes densas.  
        - N√£o requer o n√∫mero de clusters, mas depende dos par√¢metros `eps` (dist√¢ncia m√°xima) e `min_samples` (m√≠nimo de pontos por regi√£o).  
        - Funciona bem com clusters de formas arbitr√°rias e dados com ru√≠dos. 

      - **Se√ß√£o 27: Sele√ß√£o de Atributos**:  
      - **Low Variance**:  
        - Remove atributos com vari√¢ncia baixa, que pouco contribuem para a diferencia√ß√£o dos dados.  
        - √ötil para simplificar o modelo e reduzir ru√≠dos.  

      - **Extra Tree**:  
        - M√©todo baseado em √°rvores de decis√£o para calcular a import√¢ncia dos atributos.  
        - Seleciona os mais relevantes com base na divis√£o dos dados nas √°rvores.  
        - √â r√°pido e eficiente em grandes conjuntos de dados.  

    - **Se√ß√£o 28: Redu√ß√£o da Dimensionalidade**:  
      - **PCA (Principal Component Analysis)**:  
        - Transforma os dados em componentes principais que explicam a maior vari√¢ncia.  
        - Reduz a dimensionalidade enquanto preserva a maior parte da informa√ß√£o.  

      - **Kernel PCA**:  
        - Variante do PCA que usa fun√ß√µes kernel para lidar com dados n√£o lineares.  
        - Eficaz em cen√°rios onde os dados n√£o s√£o bem separados linearmente.  

      - **LDA (Linear Discriminant Analysis)**:  
        - Supervisionado, otimiza a separa√ß√£o entre classes enquanto reduz a dimensionalidade.  
        - Focado em maximizar a vari√¢ncia entre classes e minimizar a vari√¢ncia dentro das classes.  

    - **Se√ß√£o 29: Detec√ß√£o de Outliers**:  
      - **Com Boxplot**:  
        - Identifica outliers com base no intervalo interquartil (IQR).  
        - Outliers s√£o pontos fora de 1.5 vezes o IQR acima ou abaixo dos quartis.  

      - **Com gr√°fico de dispers√£o**:  
        - Visualiza poss√≠veis outliers com base na distribui√ß√£o dos dados em dois ou mais eixos.  
        - √ötil para detectar pontos an√¥malos em rela√ß√µes bivariadas.  

      - **Com PYOD**:  
        - Biblioteca Python espec√≠fica para detec√ß√£o de outliers.  
        - Oferece diversos algoritmos, como LOF, AutoEncoder e Isolation Forest, para detectar anomalias em diferentes cen√°rios.  

  - **Curso Machine Learning com Amazon AWS SageMaker**: Neste curso, foram abordados assuntos como previs√µes utilizando regress√£o linear com linear learner e xgboost, redu√ß√£o de dimensionalidade com agrupamento (clusteriza√ß√£o) com KMeans, classifica√ß√£o, detec√ß√£o de outliers com random cut, tensor flow e classifica√ß√£o de imagens com redes neurais artificiais.

      - **Se√ß√£o 03: Regress√£o com Linear Learner e XGBoost**:
        - **Linear Learner**: Algoritmo de aprendizado supervisionado utilizado para resolver problemas de regress√£o e classifica√ß√£o. Ele utiliza uma abordagem linear, ajustando os pesos das vari√°veis de entrada para minimizar o erro em rela√ß√£o √†s previs√µes.
        - **XGBoost**: Uma biblioteca de aprendizado de m√°quina que implementa √°rvores de decis√£o otimizadas por gradient boosting. √â bastante utilizada para regress√£o devido √† sua efici√™ncia e alto desempenho, mesmo em datasets grandes.

      - **Se√ß√£o 04: Classifica√ß√£o com Linear Learner e XGBoost**:
        - A diferen√ßa aqui √© que os mesmos algoritmos foram usados para resolver problemas de **classifica√ß√£o** em vez de regress√£o. Enquanto a regress√£o prev√™ valores cont√≠nuos, a classifica√ß√£o prev√™ categorias ou r√≥tulos discretos.

      - **Se√ß√£o 05: S√©ries Temporais com DeepAR**:
        - **DeepAR**: Um algoritmo de aprendizado profundo projetado para previs√µes em s√©ries temporais. Ele √© muito √∫til para lidar com conjuntos de dados com m√∫ltiplas s√©ries temporais relacionadas, aprendendo padr√µes compartilhados para melhorar a precis√£o das previs√µes.

      - **Se√ß√£o 06: Outliers com Random Cut Forest**:
        - **Random Cut Forest (RCF)**: Um algoritmo para detec√ß√£o de outliers que utiliza √°rvores baseadas em cortes aleat√≥rios para identificar outliers em dados.

      - **Se√ß√£o 07: PCA com Agrupamento K-Means**:
        - Nessa se√ß√£o, foi aplicado o **PCA (An√°lise de Componentes Principais)** para reduzir a dimensionalidade dos dados, seguido pelo agrupamento utilizando o algoritmo **K-Means**. Essa combina√ß√£o facilita a an√°lise e visualiza√ß√£o de dados complexos, agrupando-os com base em semelhan√ßas.

      - **Se√ß√£o 08: Redes Neurais Artificiais com Classifica√ß√£o de Imagem**:
        - As **Redes Neurais Artificiais (RNAs)** foram utilizadas para resolver problemas de classifica√ß√£o de imagens no SageMaker (identificar objetos ou categorias espec√≠ficas).

      - **Se√ß√£o 09: SageMaker com TensorFlow**:
        - O **TensorFlow** √© uma biblioteca de aprendizado de m√°quina amplamente utilizada para criar e treinar modelos de redes neurais. No SageMaker, foi integrado para facilitar a cria√ß√£o, o treinamento e a implanta√ß√£o de modelos de aprendizado profundo.

      - **Se√ß√£o 10: Endpoint Externo**:
        - Um **endpoint externo** criado no SageMaker permite que o modelo treinado seja acessado e utilizado em outras plataformas. Nas aulas, o endpoint foi acessado pelo Google Colab, permitindo realizar previs√µes remotamente.

      - **√öltimas Se√ß√µes: Conceitos Te√≥ricos sobre Redes Neurais**:
        - Nessas se√ß√µes, foram abordados conceitos te√≥ricos fundamentais sobre redes neurais, como arquitetura, funcionamento e tipos de redes. 

 ### Sprint 5
 Durante a Sprint 05, foi desenvolvido o projeto com objetivo de fazer previs√µes do pre√ßo de carros usados. O projeto foi dividido em:
  - **Bibliotecas Utilizadas**:
    - `Pandas`, `Numpy`, `Seaborn`, `MatplotLib`, `Shap` e `Scikit-Learn`.

  - **Amostragem dos Dados**: Foi instru√≠do que fizessemos uma amostragem dos dados do dataset `cars`. O dataset tinha aproximadamente 760 mil registros, ap√≥s ser feita uma amostragem de 25% do dataset, a nova quantidade de registros foi aproximadamente 190 mil.

  - **An√°lise Explorat√≥ria dos Dados (EDA)**: A EDA √© o processo na qual se conhece a base de dados em que voc√™ est√° trabalhando. Para analisar os 20 atributos (colunas) do dataset, foram gerados gr√°ficos como Boxplot e Histogramas para entender como estavam distribu√≠dos os dados, bem como visualizar a ocorr√™ncia de outliers. Para visualizar as vari√°veis categ√≥rias foi utilizado os gr√°ficos CountPlots da biblioteca Seaborn. Ainda na EDA, foram verificados as ocorr√™ncias dos valores nulos e tamb√©m duplicados na base de dados.

  - **Pr√©-Processamento dos Dados**: Nessa etapa, foram feitos as limpezas dos dados como tratamento de valores nulos e outliers, al√©m da transforma√ß√£o dos dados como a codifica√ß√£o de r√≥tulos, na qual foram utilizados duas t√©cnicas sendo *LabelEncoder* e *TargetEncoder*. Ainda nessa etapa, foi realizado a separa√ß√£o das vari√°veis previsoras (X) e da vari√°vel objetivo (Y). Por √∫ltimo, com o m√©todo `train_test_split()` foram separados os dados de treino, sendo 70% e os dados de teste, sendo 30%.

  - **Cria√ß√£o e Treinamento do Modelo**: Com a separa√ß√£o dos dados em treino e teste, a pr√≥xima etapa foi fazer a cria√ß√£o do modelo, utilizando o algoritmo de Random Forest, e logo em seguida foi realizado o treinamendo (m√©todo `fit()`) e as previs√µes (m√©todo `predict()`).

  - **Avalia√ß√£o e M√©tricas do Modelo**: Ap√≥s o treinamento e previs√£o, foi feito a avalia√ß√£o da performance do modelo atrav√©s das m√©tricas de `Score`, `Mean Squared Error` e `Mean Absolute Error`.

  - **Shap Values**: Por √∫ltimo, foi pedido que avaliassemos as vari√°veis que mais impactaram no modelo, atrav√©s da t√©cnica de XAI chamada *SHAP VALUES* obtive as duas vari√°veis mais importantes para o modelo.
