# üìä Entrega - Sprint 08

Nessa sprint 08, o objetivo na trilha era desenvolver um projeto de **modelagem de risco de cr√©dito** com classifica√ß√£o, utilizando machine learning e os conhecimentos adquiridos nos cursos anteriores.

# Link do V√≠deo
https://compasso-my.sharepoint.com/:v:/r/personal/emanuelle_lima_pb_compasso_com_br/Documents/sprint08-desafio3-emanuelle.lima.mp4?csf=1&web=1&nav=eyJyZWZlcnJhbEluZm8iOnsicmVmZXJyYWxBcHAiOiJPbmVEcml2ZUZvckJ1c2luZXNzIiwicmVmZXJyYWxBcHBQbGF0Zm9ybSI6IldlYiIsInJlZmVycmFsTW9kZSI6InZpZXciLCJyZWZlcnJhbFZpZXciOiJNeUZpbGVzTGlua0NvcHkifX0&e=TZvdAo

---

# üíª Projeto -  Modelagem de Risco de Cr√©dito

‚û°Ô∏è Confira o notebook do [projeto de classifica√ß√£o - modelagem de risco de cr√©dito](./modelagem-risco-credito.ipynb).

Para realizar o projeto foram necess√°rias as importa√ß√µes de algumas bibliotecas como: 
 - `pandas`, `numpy`, `seaborn`, `matplotlib`, `Pandas Profiling` e `scikit-learn`.

Al√©m disso, por quest√µes de armazenamento n√£o estarei subindo os datasets utilizados para o desenvolvimento do projeto, contudo voc√™s podem encontrar todos eles atrav√©s do link: https://www.kaggle.com/competitions/home-credit-default-risk/data 

## Entendimento dos Datasets

Para realizar o projeto, foram disponibilizados diversos datasets do **HomeCredit** que podem ser encontrados no link acima. Para entender como os datasets est√£o conectados, utilizei o *db.diagram* organizando-os atrav√©s de um identificador √∫nico presente no dataset principal, sendo:

 <p align="center">
    <img src="./data/Home Credit.png" alt="Modelo L√≥gico" width="500">
</p>

## üõ†Ô∏è Etapas do Projeto

- **Utiliza√ß√£o do Pandas Profiling**: Para ter uma vis√£o inicial dos datasets (`application_train/test`, `bureau`, `credit_card_balance` e `previous_application`), utilizei o Pandas Profiling. Essa ferramenta foi essencial para identificar a distribui√ß√£o dos dados, valores nulos, correla√ß√µes e poss√≠veis inconsist√™ncias.

- **Remo√ß√£o de Features**: Com base na an√°lise do Pandas Profiling e na visualiza√ß√£o inicial dos datasets, decidi remover colunas que:

  - Possu√≠am muitos valores nulos (defini um threshold de 40%).

  - N√£o eram fundamentais para a modelagem (na minha opini√£o).

  - Apresentavam informa√ß√µes espec√≠ficas ou redundantes (j√° presentes em outras vari√°veis).

- **Engenharia de Atributos**: Para reduzir a dimensionalidade, apliquei engenharia de atributos nas vari√°veis `ORGANIZATION_TYPE` e `CREDIT_TYPE`, agrupando categorias semelhantes e reduzindo o n√∫mero de categorias √∫nicas.

<p align="center">
    <img src="./evidencias/ev1.png" alt="Manipula√ß√£o da vari√°vel organizarion_type" width="500">
</p>

- **An√°lise Explorat√≥ria dos Dados (EDA)**: Realizei a EDA de todos os 4 datasets. O primeiro ponto a ser destacado √© que a vari√°vel `TARGET`, presente no dataset principal, est√° desbalanceada. Abaixo est√° um gr√°fico que ilustra essa despropor√ß√£o:

<p align="center"> <img src="./evidencias/target.png" alt="Countplot da Vari√°vel TARGET" width="500"> </p>

Apesar do desbalanceamento, optei por **n√£o** utilizar t√©cnicas de balanceamento devido a sua complexidade e o fato de que para o escopop do projeto em quest√£o seria a principal invi√°vel, mas utilizei o par√¢metro **stratify** no train_test_split para manter a propor√ß√£o das classes durante a divis√£o dos dados.

Durante a EDA, analisei diversas vari√°veis, contudo n√£o estarei inserindo neste documento a an√°lise completa, uma vez que eram 4 datasets diferentes com muitas vari√°veis, mas voc√™ pode encontrar a an√°lise completa no notebook do projeto. Para isso acabei dividindo-as entre num√©ricas e categ√≥ricas:

  - Para as vari√°veis num√©ricas, gerei gr√°ficos como histogramas e boxplots com o objetivo de identificar a distribui√ß√£o e se havia anomalias nas features dos datasets. Abaixo segue o exemplo da plotagem dos gr√°ficos do dataset principal:

<p align="center"> <img src="./evidencias/histogramas.png" alt="Gr√°ficos de Vari√°veis Num√©ricas" width="500"> </p>
<p align="center"> <img src="./evidencias/boxplots.png" alt="Gr√°ficos de Vari√°veis Num√©ricas" width="500"> </p>

  Al√©m disso, para entender a rela√ß√£o entre as vari√°veis num√©ricas e a vari√°vel TARGET, gerei uma matriz de correla√ß√£o de Spearman. Essa an√°lise ajudou a identificar quais vari√°veis tinham uma associa√ß√£o mais forte com a TARGET. Abaixo est√° a matriz de correla√ß√£o:

<p align="center"> <img src="./evidencias/correlation-spearman.png" alt="Matriz de Correla√ß√£o" width="600"> </p>

  - Para as vari√°veis categ√≥ricas, utilizei CountPlots e ValueCounts para entender as categorias, por exemplo:

 <p align="center">
    <img src="./evidencias/education-type.png" alt=" " width="300">
    <img src="./evidencias/income-type.png" alt=" " width="300">
    <img src="./evidencias/occupation-type.png" alt=" " width="300">
  </p>


- **Merge dos Datasets**: Posteriormente, fiz o merge entre os 4 datasets, utilizando fun√ß√µes de agrega√ß√£o nos 3 datasets secund√°rios antes de junt√°-los ao dataset principal. O merge foi realizado atrav√©s do identificador √∫nico `SK_ID_CURR`:

<p align="center"> <img src="./evidencias/merge.png" alt="Matriz de Correla√ß√£o" width="600"> </p>

Ap√≥s o merge, percebi que algumas vari√°veis possuiam uma grande quantidade de valores faltantes, dessa forma defini um novo threshold de 60% e apliquei no novo dataset gerado, sendo assim o dataset final ficou com shape de: **(307511, 56)** sendo o primeiro a quantidade de linhas e o segundo a quantidade de colunas.

- **Pr√©-processamento dos Dados**: Inicialmente, dividi meus dados em treino e teste para evitar o *data leakage*, sendo 30% dos dados para teste e 70% para treino. Depois constru√≠ um **pipeline de pr√©-processamento**, onde tratei os valores faltantes das vari√°veis num√©ricas com a *mediana* e das categ√≥ricas com a *moda*. Optei por n√£o tratar outliers, pois isso removeria muitas linhas do dataset.

<p align="center"> <img src="./evidencias/pipeline.png" alt="Pipeline" width="500"> </p>

- **Treinamento dos Modelos**: Treinei modelos de **Regress√£o Log√≠stica** e **Gradient Boosting**. O Gradient Boosting apresentou o *melhor* desempenho entre os dois, baseado nas m√©tricas mais conhecidas de classifica√ß√£o como accuracy, precision e etc.

- **M√©tricas e Avalia√ß√£o de Desempenho**: Abaixo est√£o as m√©tricas de avalia√ß√£o de desempenho de cada modelo. O Gradient Boosting errou menos, mas ambos os modelos tiveram dificuldades em prever a classe minorit√°ria (1) devido ao desbalanceamento. 

<p align="center">
    <img src="./evidencias/reg_log.png" alt="#" width="400">
    <img src="./evidencias/grad_boost.png" alt="#" width="400">
</p>

 Para analisar melhor o desempenho de ambos, foi gerada a Curva ROC, mostrando como o Gradient Boosting foi ligeraimente melhor do que a Regress√£o Log√≠stica.
<p align="center"> <img src="./evidencias/roc.png" alt="Curva ROC" width="500"> </p>


- **Teste com Novos Dados**: Para validar o modelo, realizei previs√µes com um novo conjunto de dados. A conclus√£o foi que o desbalanceamento influenciou significativamente o modelo, j√° que a maioria das previs√µes foi para a classe 0 (majorit√°ria). Abaixo est√° um gr√°fico que ilustra essa tend√™ncia:

<p align="center"> <img src="./evidencias/dadosnovos.png" alt="Previs√µes com Novos Dados" width="500"> </p>

Al√©m disso, utilizei o `predict_proba` para verificar a probabilidade que o modelo atribu√≠a a cada classe. O gr√°fico de boxplot abaixo mostra que o modelo atribui, em m√©dia, a partir de 60% de probabilidade para a classe 0 e a partir de 20% para a classe 1:

<p align="center"> <img src="./evidencias/proba.png" alt="Boxplot das Probabilidades" width="500"> </p>


Por √∫ltimo, criei um arquivo de [sample submission](./data/submission.csv), que pode ser encontrado na pasta [data](./data/)

## üéØ Conclus√£o
Este projeto de modelagem de risco de cr√©dito foi uma excelente oportunidade para aplicar conceitos de machine learning e an√°lise de dados em um problema real. Apesar dos desafios enfrentados, como o desbalanceamento da vari√°vel `TARGET` e a complexidade dos datasets, o modelo de Gradient Boosting apresentou um desempenho satisfat√≥rio, embora com limita√ß√µes na previs√£o da classe minorit√°ria.

Por fim, o projeto refor√ßou a import√¢ncia de uma an√°lise explorat√≥ria detalhada e de um pr√©-processamento cuidadoso dos dados, al√©m do fato de que o conhecimento sobre a √°rea de neg√≥cios √© fundamental para entender e resolver o problema da modelagem de risco de cr√©dito. A bagagem que esse projeto trouxe, principalmente por ser mais complexo que os demais feitos anteriormente, se tornou um aprendizando excelente e desafiador.
